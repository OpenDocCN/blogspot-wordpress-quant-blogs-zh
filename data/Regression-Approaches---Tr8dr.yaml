- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-18 15:37:42'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-18 15:37:42'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Regression Approaches | Tr8dr
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归方法 | Tr8dr
- en: 来源：[https://tr8dr.wordpress.com/2009/11/13/probabilistic-regressors-the-mean/#0001-01-01](https://tr8dr.wordpress.com/2009/11/13/probabilistic-regressors-the-mean/#0001-01-01)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://tr8dr.wordpress.com/2009/11/13/probabilistic-regressors-the-mean/#0001-01-01](https://tr8dr.wordpress.com/2009/11/13/probabilistic-regressors-the-mean/#0001-01-01)
- en: One of the readers of this blog (skauf) suggested looking into KRLS (Kernel
    Recursive Least Squares), which is an “online” algorithm for multivariate regression
    closely related to SVM and Gaussian Processes approaches.    What struck me as
    clever in the KRLS algorithm is that it incorporates an online sparsification
    approach.  Why this is important, I will attempt to explain briefly in the next
    section.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 博客的一位读者（skauf）建议研究KRLS（核递归最小二乘法），这是一种与SVM和高斯过程方法紧密相关的多变量回归“在线”算法。KRLS算法中让我觉得聪明的地方是它融入了在线稀疏化方法。为什么这很重要，我将在下一节尝试简要解释。
- en: The sparsification approach is similar in effect to PCA in that it reduces dimension
    and determines the most influential aspects of your system.  I had been thinking
    about a combined PCA / basis decomposition approach for some time, and it struck
    me that KRLS might be the way to do it.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏化方法在效果上与PCA相似，因为它减少了维度并确定了系统的最有影响力的方面。我一直在考虑一种结合PCA/基分解的方法有一段时间了，它让我觉得KRLS可能是这样做的方法。
- en: '**Kernal-based Learning Algorithms**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于核的学习算法**'
- en: 'KRLS and SVM algorithms use a common approach to classification (or regression).
      In the simplest case, let us assume that we want to find some function f(x)
    that will classify n-dimensional vectors as belonging to one of 2 sets (apples
    or oranges), which we will distinguish by + and – values from f(x):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 核递归最小二乘法（KRLS）和支撑向量机（SVM）算法在分类（或回归）方面采用了共同的方法。在最简单的情况下，假设我们想要找到某个函数f(x)，它能将n维向量分为属于两个集合（苹果或橙子）之一，我们通过f(x)的+和-值来区分这两个集合：
- en: '![Picture 1](img/d22b8b7ec3fd3aa5a8f55b14c02e3e87.png "Picture 1")'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![Picture 1](img/d22b8b7ec3fd3aa5a8f55b14c02e3e87.png "Picture 1")'
- en: Assuming the vectors X are linearly separable, we could try to find a hyperplane
    on the n-dimensional space that would separate the vectors such that there is
    a balance of distance between the two categories (and one that is maximal subject
    to some constraints).   The distance between the hyperplane (or in 2 dimensions
    a line) and the points is determined by a **margin** function.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设向量X是线性可分的，我们可以在n维空间中尝试找到一个超平面，将向量分开，使得两个类别之间的距离平衡（在某些约束条件下最大化）。超平面（或在二维中为直线）与点之间的距离由一个**间隔**函数确定。
- en: '![Picture 2](img/13949f33cf0cd536904567375db641e6.png "Picture 2")'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![Picture 2](img/13949f33cf0cd536904567375db641e6.png "Picture 2")'
- en: 'Optimization is accomplished by maximizing the **margins** subject to constraints.
     The optimization solves for the weights α  such that the sum of the weighted
    dot product of  X with each Xi yields the predicted value.  The form of f(x) is
    the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 优化通过最大化**间隔** subject to 约束来实现。优化解决权重α，使得X与每个Xi的加权点积之和得出预测值。f(x)的形式如下：
- en: '![Picture 3](img/bf883c4a27e5ad16bad7bdcbe0ea64f0.png "Picture 3")'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![Picture 3](img/bf883c4a27e5ad16bad7bdcbe0ea64f0.png "Picture 3")'
- en: Now the above graph shows a very well-behaved data set, with a clear boundary
    for classification.  Many data sets, however, will have significant noise in the
    data and/or outliers that refuse to be categorized correctly.   This data has
    an overall impact on the regression line.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，上述图表显示了一个表现非常好的数据集，有一个清晰的分类边界。然而，许多数据集可能存在数据中的显著噪声和/或拒绝正确分类的异常值。这些数据对回归线有整体影响。
- en: Supposing we have N samples in our training set {{X1,Y1}, {X2,Y2}, … {Xn, Yn}}.
      In simple terms, sparsification is the process of removing (or discounting)
    samples that are already represented in the existing set, reducing the bias of
    the regressor.   One approach to determining the degree of representation is to
    observe the “degree of orthogonality” represented by a given vector with respect
    to the existing set.   Sparsification also points to an approach to evolving the
    regressor over time relative to new observations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有N个样本在训练集{{X1,Y1}, {X2,Y2}, … {Xn, Yn}}中。简单来说，稀疏化过程是移除（或折扣）现有集合中已经表示的样本，从而减少回归器的偏差。确定表示程度的一种方法是观察给定向量与现有集合之间的“正交度”。稀疏化还指向了一种根据新观察值相对时间演进回归器的方法。
- en: '**The Kernel'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**核函数**'
- en: Above gave a brief overview of how SVM uses the notion of margin to classify
    data.  A kernel is not necessary for data that is linearly separable.   The kernel
    is “simply” a function that maps data from the “attribute” space to “feature”
    space.   We design or choose the kernel function so that our data is largely linearly
    separable in the “feature” space and with the constraint that the covariance matrix
    of all possible vectors mapped from attribute to feature space will be positive
    semi-definite.**
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 上面简要介绍了SVM如何利用间隔概念来分类数据。对于线性可分的数据，核函数并非必需。核函数“简单”地是一个将数据从“属性”空间映射到“特征”空间的函数。我们设计或选择核函数，以便我们的数据在“特征”空间中尽可能线性可分，并且所有可能从属性空间映射到特征空间的向量的协方差矩阵为正定半矩阵。**
- en: '**Since our linear SVM equations are expressed in terms of inner products,
    given a feature mapping function Φ(x) mapping X from non-linear space to “linearly
    separable space”, we can express the kernel as a function of the inner product
    of two vectors, later to plug in to our linear equations:**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**由于我们的线性SVM方程是用内积来表达的，给定一个特征映射函数 Φ(x)，将X从非线性空间映射到“线性可分空间”，我们可以将核函数表示为两个向量内积的函数，然后插入到我们的线性方程中：**'
- en: '**![Picture 5](img/54e0ea521ec9b537f4781da94b1baeb1.png "Picture 5")**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**![Picture 5](img/54e0ea521ec9b537f4781da94b1baeb1.png "Picture 5")**'
- en: '**The trick is to choose a kernel function that maximizes dispersion of the
    data into sets that are linearly separable.**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键是选择一个能最大化数据分散到线性可分集合的核函数。**'
- en: '****How does this relate to an explicit probability based regression?**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '****这如何与显式的基于概率的回归相关？**'
- en: It turns out that the process of optimizing the margin function on a Gaussian
    kernel  is equivalent to finding the unnormalized maximum likelihood, whereas
    the Gaussian Process approach makes this explicit.**
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，在高斯核上优化间隔函数的过程等价于找到未归一化的最大似然性，而高斯过程方法使这一点变得明确。**
- en: '****Which model does one choose?** The Gaussian Process approach is strictly
    bayesian.   It has the upside of providing explicit probabilities / confidence
    measures on predictions.  If one knows the likelihoods of the desired labels with
    respect to the attribute vectors, the model works very well and provides more
    information than the SVM family.**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '****选择哪种模型？** 高斯过程方法是严格的贝叶斯方法。它提供了显式的概率/置信度量。如果知道所需标签相对于属性向量的似然，该模型表现非常好，并比SVM家族提供更多信息。**'
- en: '**The SVM family of approaches use a margin function to determine the similarity
    between vectors (for the purpose of classification).   This does not explicitly
    involve probabilities, but for the Gaussian kernel can be shown to be equivalent
    to the Gaussian Process approach.   The SVM family has the upside that one does
    not need to know the likelihoods.**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**SVM方法家族使用间隔函数来确定向量之间的相似性（用于分类）。这并不直接涉及概率，但对于高斯核可以证明与高斯过程方法等价。SVM家族的优势在于，不需要知道似然函数。**'
- en: '**Finally, both models allow the use of kernels to map data from a non-normal
    or non-linear space to a linear or gaussian space.   Some have shown that there
    is a degree of equivalence between the likelihood function in the Gaussian Process
    algorithm and the kernel function used in SVM.**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**最后，这两种模型都允许使用核函数将数据从非正常或非线性空间映射到线性或高斯空间。一些研究表明，在高斯过程算法中的似然函数和SVM中使用的核函数之间存在一定程度的等价性。**'
