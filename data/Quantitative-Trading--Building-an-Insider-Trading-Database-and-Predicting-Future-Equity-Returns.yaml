- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-05-12 18:56:03'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-12 18:56:03
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Quantitative Trading: Building an Insider Trading Database and Predicting Future
    Equity Returns'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化交易：构建内部交易数据库并预测未来股票回报
- en: 来源：[http://epchan.blogspot.com/2017/07/building-insider-trading-database-and.html#0001-01-01](http://epchan.blogspot.com/2017/07/building-insider-trading-database-and.html#0001-01-01)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[http://epchan.blogspot.com/2017/07/building-insider-trading-database-and.html#0001-01-01](http://epchan.blogspot.com/2017/07/building-insider-trading-database-and.html#0001-01-01)
- en: ===
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ===
- en: I’ve long been interested in the behavior of corporate insiders and how their
    actions may impact their company’s stock. I had done some research on this in
    the past, albeit in a very low-tech way using mostly Excel. It’s a highly compelling
    subject, intuitively aligned with a company’s equity performance - if those individuals
    most in-the-know are buying, it seems sensible that the stock should perform well.
    If insiders are selling, the opposite is implied. While reality proves more complex
    than that, a tremendous amount of literature has been written on the [topic](https://scholar.google.com/scholar?hl=en&q=insider+trading&btnG=&as_sdt=1%2C22&as_sdtp=),
    and it has shown to be predictive in [prior studies](https://www.amazon.com/Investment-Intelligence-Insider-Trading-Press/dp/0262692341/ref=sr_1_1?ie=UTF8&qid=1500148884&sr=8-1&keywords=nejat+seyhun).In
    generating [my thesis](https://urldefense.proofpoint.com/v2/url?u=https-3A__papers.ssrn.com_sol3_papers.cfm-3Fabstract-5Fid-3D3000704&d=DwMCaQ&c=yHlS04HhBraes5BQ9ueu5zKhE7rtNXt_d012z2PA6ws&r=HcdzcN9QHYqnVIfkU6GOG0i6ty8DOaZLP8DKHa427Ac&m=Q7q5j0CJlcGtx3XyIuy1mlF4VfgYniHcin8TiQRcmQE&s=4fOizzmREE5lyEOeySMpziy2xLR3UIbGR4to1_YRdJw&e=)
    to complete Northwestern’s MS in Predictive Analytics program, I figured employing
    some of the more prominent machine learning algorithms to insider trading could
    be an interesting exercise. I was concerned, however, that, as the market had
    gotten smarter over time, returns from insider trading signals may have decayed
    as well, as is often the case with strategies exposed to a wide audience over
    time. Information is more readily available now than at any time in the past.
    Not too long ago, investors needed to visit SEC offices to obtain insider filings.
    The standard filing document, the form 4 has only required electronic submission
    since 2003\. Now anyone can obtain it freely via the [SEC’s EDGAR website](https://www.sec.gov/edgar/searchedgar/companysearch.html).
    If all this data is just sitting out there, can it continue to offer value?I decided
    to inquire by gathering the filings directly by scraping the EDGAR site.  While
    there are numerous data providers available (at a cost), I wanted to parse the
    raw data directly, as this would allow for greater “intimacy” with the underlying
    data. I’ve spent much of my career as a database developer/administrator, so working
    with raw text/xml and transforming it into a database structure seemed like fun.
    Also, since I desired this to be a true end-to-end data science project, including
    the often ugly [80% of the real effort](http://blog.revolutionanalytics.com/2014/08/data-cleaning-is-a-critical-part-of-the-data-science-process.html)
    – data wrangling, was an important requirement.  That being said, mining and cleansing
    the data was a monstrous amount of work. It took several weekends to work through
    the code and finally download 2.4 million unique files. I relied heavily on Powershell
    scripts to first parse through the files and shred the xml into database tables
    in MS SQL Server.With data from the years 2005 to 2015, the initial 2.4 million
    records were filtered down to 650,000 Insider Equity Buy transactions. I focused
    on Buys rather than Sells because the signal can be a bit murkier with sells.
    Insider selling happens for a great many innocent reasons, including diversification
    and paying living expenses. Also, I focused on equity trades rather than derivatives
    for similar reasons -it can be difficult to interpret the motivations behind various
    derivative trades.  Open market buy orders, however, are generally quite clear.After
    some careful cleansing, I had 11 years’ worth of useful SEC data, but in addition,
    I needed pricing and market capitalization data, ideally which would account for
    survivorship bias/dead companies. Respectively, Zacks Equity Prices and Sharadar’s
    Core US Fundamentals data sets did the trick, and I could obtain both via [Quandl](https://www.quandl.com/)
    at reasonable cost (about $350 per quarter.)For exploratory data analysis and
    model building, I used the R programming language. The models I utilized were
    linear regression, recursive partitioning, random forest and multiplicative adaptive
    regression splines (MARS).  I intended to make use of a support vector machine
    (SVM) models as well, but experienced a great many performance issues when running
    on my laptop with a mere 4 cores. SVMs have trouble with scaling. I failed to
    overcome this issue and abandoned the effort after 10-12 crashes, unfortunately.For
    the recursive partitioning and random forest models I used functions from Microsoft’s
    RevoScaleR package, which allows for impressive scalability versus standard tree-based
    packages such as rpart and randomForest. Similar results can be expected, but
    the RevoScaleR packages take great advantage of multiple cores. I split my data
    into a training set for 2005-2011, a validation set for 2012-2013, and a test
    set for 2014-2015\. Overall, performance for each of the algorithms tested were
    fairly similar, but in the end, the random forest prevailed.For my response variable,
    I used 3-month relative returns vs the Russell 3000 index. For predictors, I utilized
    a handful of attributes directly from the filings and from related company information.
    The models proved quite predictive in the validation set as can be seen in exhibit
    4.10 of the paper, and reproduced below:The random forest’s predicted returns
    were significantly better for quintile 5, the highest predicted return grouping,
    relative to quintile 1(the lowest). Quintiles 2 through 4 also lined up perfectly
    - actual performance correlated nicely with grouped predicted performance.  The
    results in validation seemed very promising!However, when I ran the random forest
    model on the test set (2014-2015), the relationship broke down substantially,
    as can be seen in the paper’s Exhibit 5.2, reproduced below:Fortunately, the predicted
    1^(st) decile was in in fact the lowest performing actual return grouping. However,
    the actual returns on all remaining prediction deciles appeared no better than
    random. In addition, relative returns were negative for every decile.  While disappointing,
    it is important to recognize that when modeling time-dependent financial data,
    as the time-distance moves further away from the training set’s time-frame, performance
    of the model tends to decay. All market regimes, gradually or abruptly, end. This
    represents a partial (yet unsatisfying) explanation for this relative decrease
    in performance. Other effects that may have impaired prediction include the use
    of price, as well as market cap, as predictor variables. These factors certainly
    underperformed during the period used for the test set. Had I excluded these,
    and refined the filing specific features more deeply, perhaps I would have obtained
    a clearer signal in the test set.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我长期以来一直对公司内部人士的行为以及他们的行为可能如何影响公司股价感兴趣。我过去在这方面做了一些研究，尽管使用的是非常低科技的方法，主要是Excel。这是一个高度引人入胜的主题，直观上与公司的股权表现相一致——如果那些最知情的人在购买，似乎股票表现良好是有道理的。如果内部人士在出售，则暗示相反。现实证明比这更复杂，但已经有大量文献对这个[主题](https://scholar.google.com/scholar?hl=en&q=insider+trading&btnG=&as_sdt=1%2C22&as_sdtp=)进行了研究，并在[先前的研究](https://www.amazon.com/Investment-Intelligence-Insider-Trading-Press/dp/0262692341/ref=sr_1_1?ie=UTF8&qid=1500148884&sr=8-1&keywords=nejat+seyhun)中显示是具有预测性的。为了完成西北大学的预测分析硕士项目，我想到运用一些更为突出的机器学习算法来研究内部人交易可能会是一个有趣的练习。然而，我担心随着时间的推移，市场变得更加聪明，内部交易信号的回报可能也已经衰减，正如随着时间的推移被广大观众知晓的策略一样。现在比过去任何时候信息都更容易获得。不久前，投资者需要访问证券交易委员会（SEC）的办公室以获取内部文件。标准提交文件，即表格4，自2003年起只要求电子提交。现在任何人都可以通过[SEC的EDGAR网站](https://www.sec.gov/edgar/searchedgar/companysearch.html)免费获得。如果所有这些数据都只是放在那里，它还能继续提供价值吗？我决定通过直接从EDGAR网站抓取文件来查询。尽管有许多数据提供者可供选择（需要支付费用），但我想要直接解析原始数据，因为这将允许与底层数据有更大的“亲密”接触。我在职业生涯的大部分时间里担任数据库开发人员/管理员，所以处理原始文本/xml并将它们转化为数据库结构似乎很有趣。此外，因为我希望这成为一个真正的端到端数据科学项目，包括通常看起来不那么美观的[80%的实际努力](http://blog.revolutionanalytics.com/2014/08/data-cleaning-is-a-critical-part-of-the-data-science-process.html)——数据整理，这是一个重要的要求。说到这一点，挖掘和整理数据是一项庞大的工作。我花了几个周末来编写代码，最终下载了240万个独特的文件。我严重依赖PowerShell脚本来首先解析文件并将XML
    shredded到MS SQL Server的数据库表中。利用2005年至2015年的数据，最初的240万个记录被过滤 down to 650,000个内部人股权购买交易。我之所以关注购买而非出售，是因为出售的信号可能会有些模糊。内部人出售有无数个无辜的理由，包括分散投资和支付生活费用。此外，我之所以关注股权交易而非衍生品交易，也是因为类似的原因
    - 解读各种衍生品交易背后的动机可能很困难。然而，公开市场的购买订单通常是非常清晰的。经过一些仔细的清理后，我拥有了11年的有用SEC数据，但除此之外，我还需要定价和市值数据，最好是考虑到生存偏差/已破产公司的数据。Zacks股权价格和Sharadar的核心美国基本面数据集解决了这个问题，而且我可以通过[Quandl](https://www.quandl.com/)以合理的价格（大约每个季度$350）获得这两个数据集。为了探索性数据分析模型建立，我使用了R编程语言。我所使用的模型包括线性回归、递归分割、随机森林和乘性适应性回归样条（MARS）。我本打算使用支持向量机（SVM）模型，但在我的笔记本上运行时遇到了性能问题，因为我的笔记本只有4个核心。SVM在缩放方面有困难。不幸的是，我在经历了10-12次崩溃后，未能克服这个问题，不得不放弃努力。对于递归分割和随机森林模型，我使用了来自微软RevoScaleR包的函数，该包允许与标准基于树的包（如rpart和randomForest）相比具有令人印象深刻的可扩展性。可以期待类似的结果，但RevoScaleR包充分利用了多个核心。我将我的数据分为2005-2011年的训练集、2012-2013年的验证集以及2014-2015年的测试集。尽管测试集中每个算法的表现相当相似，但最终随机森林胜
- en: In any event, this was a fun exercise where I learned a great deal about insider
    trading and its impact on future returns. Perhaps we can conclude that this signal
    has weakened over time, as the market has absorbed the informational value of
    insider trading data. However, perhaps further study, additional feature engineering
    and clever consideration of additional algorithms is worth pursuing in the future.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，这是一个很有趣的练习，我从中学习到了很多关于内部交易及其对未来回报影响的知识。或许我们可以得出这样的结论：随着时间的推移，这一信号已经减弱，因为市场已经吸收了内部交易数据的
    informational value。然而，或许未来进一步的研究、额外的特征工程和巧妙地考虑额外的算法是值得追求的。
- en: '*John J Ryle, CFA lives in the Boston area with his wife and two children.
    He is a software developer at a hedge fund, a graduate of Northwestern’s Master’s
    in Predictive Analytics program (2017), a huge tennis fan, and a machine learning
    enthusiast. He can be reached at john@jryle.com. *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰·J·雷尔，CFA与妻子和两个孩子一起居住在波士顿地区。他在一家对冲基金担任软件开发人员，是西北大学预测分析硕士课程（2017年）的毕业生，是一位巨大的网球爱好者，也是机器学习的热心者。您可以通过john@jryle.com联系到他。*'
- en: ===
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ===
- en: '**Upcoming Workshops by Dr. Ernie Chan**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**陈博士即将举办的研讨会**'
- en: In the last few years, mean reversion strategies have proven to be the most
    consistent winner. However, not all mean reversion strategies work in all markets
    at all times. This workshop will equip you with basic statistical techniques to
    discover mean reverting markets on your own, and describe the detailed mechanics
    of trading some of them.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，均值回归策略已被证明是最稳定的赢家。然而，并不是所有的均值回归策略在所有的市场和所有的时间都有效。本次研讨会将为您提供基本的统计技术，让您独自发现均值回归市场，并描述其中一些交易的详细机制。
- en: ===
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ===
- en: Industry updates
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 行业更新
- en: scriptmaker.net allows users to record order book data for backtesting.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 【脚本制造商】(https://scriptmaker.net/) 允许用户记录订单簿数据用于回测。
- en: '[Pair Trading Lab](https://www.pairtradinglab.com/) offers a web-based platform
    for easy backtesting of pairs strategies.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 【成对交易实验室】(https://www.pairtradinglab.com/) 提供了一个基于网页的平台，便于对成对交易策略进行简单的回测。
