- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-12 18:54:55'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-12 18:54:55'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Quantitative Trading: Experiments with GANs for Simulating Returns (Guest post)'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化交易：使用GANs模拟回报的实验（嘉宾文章）
- en: 来源：[http://epchan.blogspot.com/2019/12/experiments-with-gans-for-simulating.html#0001-01-01](http://epchan.blogspot.com/2019/12/experiments-with-gans-for-simulating.html#0001-01-01)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[http://epchan.blogspot.com/2019/12/experiments-with-gans-for-simulating.html#0001-01-01](http://epchan.blogspot.com/2019/12/experiments-with-gans-for-simulating.html#0001-01-01)
- en: 2) The discriminator is able to tell real data from the generated one
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 判别器能够区分真实数据和生成的数据
- en: 'The mathematical objectives of this training are to maximise:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这次训练的数学目标是为了最大化：
- en: a ) log(D(x)) + log(1 - D(G(z))) - Done by the discriminator - Increase the
    expected ( over many iterations ) log probability of the Discriminator D to identify
    between the real and fake samples x. Simultaneously, increase the expected log
    probability of discriminator D to correctly identify all samples generated by
    generator G using noise z.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: a) log(D(x)) + log(1 - D(G(z))) - 由判别器完成 - 增加判别器D识别真实和假样本x的期望对数概率。同时，增加判别器D正确识别由生成器G使用噪声z生成的所有样本的期望对数概率。
- en: b)  log(D(G(z))) - Done by the generator - So, as observed empirically while
    training GANs, at the beginning of training G is an extremely poor “truth” generator
    while D quickly becomes good at identifying real data. Hence, the component log(1
    - D(G(z))) saturates or remains low. It is the job of G to maximize log(1 - D(G(z))).
    What that means is G is doing a good job of creating real data that D isn’t able
    to “call out”. But because log(1 - D(G(z))) saturates, we train G to maximize
    log(D(G(z))) rather than minimize log(1 - D(G(z))).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: b) log(D(G(z))) - 由生成器完成 - 因此，如在训练GANs时观察到的，在训练的开始阶段，G是一个非常差的“真实”生成器，而D很快就能很好地识别真实数据。因此，组件log(1
    - D(G(z)))饱和或保持较低。生成器G的任务是最大化log(1 - D(G(z)))。这意味着G正在创造D无法“指出”的真实数据。但是因为log(1
    - D(G(z)))饱和，我们训练G以最大化log(D(G(z)))，而不是最小化log(1 - D(G(z)))。
- en: 'Together the min-max game that the two networks play between them is formally
    described as:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 两个网络之间进行的min-max游戏正式描述为：
- en: minGmaxDV (D, G) =Epdata(x)[log D(x)]  +E p(z) [log(1 − D(G(z)))]
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: minGmaxDV (D, G) =Epdata(x)[log D(x)]  +E p(z) [log(1 − D(G(z)))]
- en: The real data sample x is sampled from the distribution of empirical returns
    pdata(x)and the zis random noise variable sampled from a multivariate gaussian
    p(z). The expectations are calculated over both these distributions. This happens
    over multiple iterations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 真实数据样本 x 从经验回报分布 pdata(x) 中采样，而 z 是从多元高斯分布 p(z) 中采样出的随机噪声变量。期望值是对这两个分布进行计算的。这个过程会在多次迭代中进行。
- en: The hypothesis was that the various GANs tried will be able to generate a distribution
    of returns which are closer to the empirical distributions of returns than ubiquitous
    baselines like Monte Carlo method using the [Geometric Brownian motion](https://blog.quantinsti.com/random-walk-geometric-brownian-motion/).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 假设是各种尝试的GAN将能够生成一个回报分布，这个分布比使用[几何布朗运动](https://blog.quantinsti.com/random-walk-geometric-brownian-motion/)的蒙特卡洛方法等普遍基线更接近于经验回报分布。
- en: The experiments
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 实验
- en: A bird’s-eye view of what we’re trying to do here is that we’re trying to learn
    a joint probability distribution across time windows of all features along with
    the percentage change in adjusted close. This is so that they can be simulated
    organically with all the nuances they naturally come together with. For all the
    GAN training processes, Bayesian optimisation was used for hyperparameter tuning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图从所有特性的时间窗口以及调整收盘价的百分比变化学习一个联合概率分布。这样，它们可以有机地结合所有自然的细微差别进行模拟。在所有GAN训练过程中，都使用了贝叶斯优化进行超参数调整。
- en: In this exercise, initially, we first collected some features belong to the
    categories of trend, momentum, volatility etc like [RSI](https://blog.quantinsti.com/rsi-indicator/),
    MACD, [Parabolic SAR](https://blog.quantinsti.com/parabolic-sar/), [Bollinger
    bands](https://blog.quantinsti.com/bollinger-bands/) etc to create a feature set
    on the adjusted close of AAPL data which spanned from the 1980s to today. The
    window size of the sequential training sample was set based on hyperparameter
    tuning. Apart from these indicators the percentage change in the adjusted OLHCV
    data were taken and concatenated to the list of features. Both the generator and
    discriminator were recurrent neural networks ( to sequentially take in the multivariate
    window as input) powered by LSTMs which further passed the output to dense layers.
    I have tried learning the joint distributions of 14 and also 8 features The results
    were suboptimal,  probably because of the architecture being used and also because
    of how notoriously tough the GAN architecture might become to train. The suboptimality
    was in terms of the generators’ error not reducing at all ( log(1 - D(G(z))) saturating
    very early in the training ) after initially going up and the random return distributions
    without any particular form being generated by the generators.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，起初，我们首先收集了一些属于趋势、动量、波动性等类别的特征，如[RSI](https://blog.quantinsti.com/rsi-indicator/)、MACD、[帕abolic
    SAR](https://blog.quantinsti.com/parabolic-sar/)、[布林带](https://blog.quantinsti.com/bollinger-bands/)等，以创建AAPL数据的特征集，该数据集从20世纪80年代至今。序列训练样本的窗口大小是基于超参数调优设定的。除了这些指标外，调整后的OLHCV数据的百分比变化也被采取并连接到特征列表中。生成器和判别器都是递归神经网络（按顺序输入多变量窗口），由LSTM提供动力，并将输出传递给密集层。我尝试学习14个特征和8个特征的联合分布。结果并不理想，可能是因为所使用的架构，也可能是因为GAN架构训练起来非常困难。不理想之处在于生成器的误差完全没有减少（log(1
    - D(G(z)))在训练初期上升后非常早地达到饱和），而且生成器产生的随机回报分布没有任何特定的形式。
- en: After trying conditional recurrent GANs, which didn’t train well, I tried using
    simpler multilayer perceptrons for both Generator and Discriminators in which
    I passed the entire window of returns of the adjusted close price of AAPL. The
    optimal window size was derived from hyperparameter tuning using Bayesian optimisation.
    The distribution generated by the feed-forward GAN is shown in figure 1.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试了训练效果不佳的条件循环GAN之后，我尝试使用更简单的多层感知机作为生成器和判别器，我将AAPL调整后的收盘价的全部窗口返回值传递给生成器。最优窗口大小是通过使用贝叶斯优化进行超参数调优得到的。前馈GAN生成的分布如图1所示。
- en: Fig 1\. Returns by simple feed-forward GAN
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1。简单前馈GAN的回报
- en: Some of the common problems I faced were either partial or complete [mode collapse](https://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/)
    - where the distribution either did not have a similar sharp peak as the empirical
    distribution ( partial ) or any noise sample input into the generator produces
    a limited set of output samples ( complete).        ![](img/a0bcb3713296408cea79e497d17cbc0c.png)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我遇到的一些常见问题要么是部分[模式坍缩](https://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/)，要么是完全模式坍缩。在这两种情况下，分布要么没有与经验分布类似的尖锐峰值（部分），要么无论噪声样本输入生成器产生的是一个有限的输出样本集（完全）。![](img/a0bcb3713296408cea79e497d17cbc0c.png)
- en: 'The figure above shows mode collapsing during training. Every subsequent epoch
    of the training is printed with the mean and standard deviation of both the empirical
    subset (“real data”) that is put into the discriminator for training and the subset
    generated by the generator ( “fake data”). As we can see at the 150th epoch, the
    distribution of the generated “fake data” absolutely collapses. The mean becomes
    1.0 and the stdev becomes 0\. What this means is that all the noise samples put
    into the generator are producing the same output! This phenomenon is called Mode
    Collapse as the frequencies of other local modes are not inline with the real
    distribution. As you can see in the figure below, this is the final distribution
    generated in the training iterations shown above:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图表显示了训练过程中的模式坍缩。每次训练的后续周期都会打印出放入判别器进行训练的经验子集（“真实数据”）和生成器生成的子集（“假数据”）的平均值和标准差。正如我们在第150个周期所看到的，生成的“假数据”的分布完全坍缩。均值变为1.0，标准差变为0。这意味着生成器中放入的所有噪声样本都产生了相同的输出！这种现象称为模式坍缩，因为其他局部模式的频率与真实分布不一致。正如您在下面的图表中看到的，这是上面训练迭代中生成的最终分布：
- en: A few tweaks which reduced errors for both Generator and Discriminator were
    1) using a different learning rate for both the neural networks. Informally, the
    discriminator learning rate should be one order higher than the one for the generator.
    2) Instead of using fixed labels like 1 or a 0 (where 1 means “real data” and
    0 means “fake data”) for training the discriminator it helps to subtract a small
    noise from the label 1 and add a similar small noise to label 0\. This has the
    effect of changing from classification to a regression model, using mean square
    error loss instead of binary cross-entropy as the objective function. Nonetheless,
    these tweaks have not eliminated completely the suboptimality and mode collapse
    problems associated with recurrent networks.
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对生成器和判别器的错误进行了少量调整，1) 为两个神经网络使用了不同的学习率。非正式地说，判别器的学习率应该是生成器学习率的一个数量级更高。2) 训练判别器时，不是使用固定的标签1或0（其中1表示“真实数据”，0表示“假数据”），而是从标签1中减去一小部分噪声，并从标签0中添加类似的一小部分噪声。这使得从分类转变为回归模型，使用均方误差损失而不是二元交叉熵作为目标函数。尽管如此，这些调整并没有完全消除与循环网络相关的次优性和模式崩溃问题。
- en: Baseline Comparisons
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基线比较
- en: We compared this generated distribution against the distribution of empirical
    returns and the distribution generated via the Geometric Brownian Motion - [Monte
    Carlo](https://blog.quantinsti.com/introduction-monte-carlo-analysis/) simulations
    done on AAPL via python. The metrics used to compare the empirical returns from
    GBM-MC and GAN were Kullback-Leibler divergence to compare the “distance” between
    return distributions and VAR measures to understand the risk being inferred for
    each kind of simulation. The chains generated by the GBM-MC can be seen in fig.
    4\. Ten paths were simulated in 1000 days in the future based on the inputs of
    the variance and mean of the AAPL stock data from the 1980s to 2019\. The input
    for the initial price in GBM was the AAPL price on day one.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将生成的分布与经验回报分布和通过几何布朗运动生成的分布进行了比较- 对AAPL的Monte Carlo模拟使用python完成的。用于比较GBM-MC生成的经验回报和GAN生成的回报的度量是Kullback-Leibler散度来比较回报分布之间的“距离”和VAR测量来了解每种模拟的风险。GBM-MC生成的链可以在图4中看到。基于AAPL股票数据的方差和均值从20世纪80年代到2019年，在未来的1000天内模拟了10个路径。GBM的初始价格输入是第一天的AAPL价格。
- en: '![](img/115da8c11b0108ec9d3e9cf8efb897cd.png)  ![](img/8e91fb75435aa0cd43d07ad00bccb30f.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/115da8c11b0108ec9d3e9cf8efb897cd.png)  ![](img/8e91fb75435aa0cd43d07ad00bccb30f.png)'
- en: Fig 2\. shows the empirical distributions for AAPL starting 1980s up till now.
    Fig 3\. shows the generated returns by Geometric Brownian motion on AAPL.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2显示了从20世纪80年代至今的AAPL的经验分布。图3显示了AAPL上几何布朗运动的生成回报。
- en: 'To compare the various distributions generated in the exercise I binned the
    return values into 10,000 bins and then calculated the Divergence using the non-normalised
    frequency value of each bin. The code is:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较练习中生成的各种分布，我将回报值分为10,000个桶，然后使用每个桶的非标准化频率值计算散度。代码是：
- en: 'The formula scipy uses behind the scene for entropy is:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 背后使用的scipy的熵公式是：
- en: S = sum(pk * log(pk / qk)) where pk,qk are bin frequencies
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: S = sum(pk * log(pk / qk)) 其中pk,qk是二进制频率
- en: '| Comparison | KL Divergence |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 比较 | KL散度 |'
- en: '| Empirical vs GAN | 7.155841564194154 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 经验值与GAN | 7.155841564194154 |'
- en: '| GAN vs Empirical  | 10.180867728820251 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| GAN与经验值 | 10.180867728820251 |'
- en: '| Empirical vs GBM  | 1.9944835997277586 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 经验值与GBM | 1.9944835997277586 |'
- en: '| GBM vs Empirical  | 2.990622397328334 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| GBM与经验值 | 2.990622397328334 |'
- en: The Geometric Brownian Motion generation is a better match for the empirical
    data compared to the one generated using Multiperceptron GANs even though it should
    be noted that both are extremely bad.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用多感知机GANs生成的回报相比，几何布朗运动生成更好地与经验数据相匹配，尽管需要注意的是两者都非常糟糕。
- en: 'The VAR values ( calculated over 8 samples ) here tell us that beyond a confidence
    level, the kind of returns (or losses) we might get - in this case, it is the
    percentage losses with 5% and 1% chance given the distributions of returns:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里计算的VAR值（跨越8个样本）告诉我们，在置信水平以上，我们可能获得的回报（或损失）的类型- 在这个案例中，是给出回报分布的百分比损失，5%和1%的机会：
- en: '| Comparison | Mean and Std Dev of VAR Values ( for 95% confidence level ) 
    | Mean and Std Dev of VAR Values ( for 99% confidence level ) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 比较 | VAR值的均值和标准差（对于95%置信水平）| VAR值的均值和标准差（对于99%置信水平）|'
- en: '| GANs | Mean = -0.1965352900Stdev =  0.007326252 | Mean = -0.27456501573Stdev
    =  0.0093324205 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| GANs | Mean = -0.1965352900Stdev =  0.007326252 | Mean = -0.27456501573Stdev
    =  0.0093324205 |'
- en: '| GBM with Monte Carlo  | Mean = -0.0457949236Stdev =  0.0003046359 | Mean
    = -0.0628570539Stdev = 0.0008578205 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| GBM with Monte Carlo  | Mean = -0.0457949236Stdev =  0.0003046359 | Mean
    = -0.0628570539Stdev = 0.0008578205 |'
- en: '| Empirical data | -0.0416606773394755 (one ground truth value)  | -0.0711425634927405
    (one ground truth value)  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 经验数据 | -0.0416606773394755 (one ground truth value)  | -0.0711425634927405
    (one ground truth value)  |'
- en: The GBM generator VARs seem to be much closer to the VARs of the Empirical distribution.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: GBM生成器的VAR似乎比经验分布的VAR更接近。
- en: . Fig 4\. Showing the various paths generated by the Geometric Brownian motion
    model using monte Carlo.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图4. 显示使用蒙特卡洛的Geometric Brownian motion模型生成的各种路径。
- en: Conclusion
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论**'
- en: The distributions generated by both methods didn’t generate the sharp peak shown
    in the empirical distribution (figure 2). The spread of the return distribution
    by the GBM with Monte Carlo was much closer to reality as shown by the VAR values
    and its distance to the empirical distribution was much closer to the empirical
    distribution as shown by the Kulback-Leibler divergence, compared to the ones
    generated by the various GANs I tried. This exercise reinforced that GANs even
    though enticing are tough to train. While at it I discovered and read about a
    few tweaks that might be helpful in GAN training. Some of the common problems
    I faced were 1) mode collapse discussed above 2) Another one was the saturation
    of the generator and “overpowering” by the discriminator. This saturation causes
    suboptimal learning of distribution probabilities by the GAN. Although not really
    successful, this exercise creates scope for exploring the various newer GAN architectures,
    in addition to the conditional recurrent and multilayer perceptron ones which
    I tried, and use their fabled ability to learn the subtlest of distributions and
    apply them for financial time-series modelling. Our codes can be found at Github
    [here](https://github.com/QuantInsti/EPAT/tree/master/Blogs/GAN%20Simulation).
    Any modifications to the codes that can help improve performance are most welcome!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法生成的分布都没有生成经验分布（图2）中所显示的尖锐峰。与我所尝试的各种GANs相比，使用GBM with Monte Carlo生成的回报分布更加接近现实，如VAR值所示，其与经验分布的距离也远小于经验分布，如Kullback-Leibler散度所示。这次练习强化了这样一个观点：尽管GANs很有吸引力，但它们很难训练。在这个过程中，我发现了几个可能在GAN训练中有帮助的小技巧。我所面临的一些常见问题包括上面讨论过的1)模式坍塌
    2)生成器的过饱和和“过度强化”判别器。这种过饱和导致GAN学习概率分布的最优性降低。尽管这次练习并不特别成功，但它为探索各种新的GAN架构以及我尝试的条件递归和多层感知器架构提供了可能，并利用它们传说中的能力学习最微妙的分布，并将它们应用于金融时间序列建模。我们的代码可以在Github上找到[这里](https://github.com/QuantInsti/EPAT/tree/master/Blogs/GAN%20Simulation)。任何能帮助提高性能的代码修改都是最受欢迎的！
- en: 'About Author:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于作者**：'
- en: Akshay Nautiyal is a Quantitative Analyst at Quantinsti, working at the confluence
    of Machine Learning and Finance. QuantInsti is a premium institute in Algorithmic
    & Quantitative Trading with [instructor-led](https://www.quantinsti.com/epat)
    and [self-study](https://quantra.quantinsti.com/courses) learning programs. For
    example, there is an interactive [course](https://quantra.quantinsti.com/learning-track/machine-learning-deep-learning-in-financial-markets)
    on using Machine Learning in Finance Markets that provides hands-on training in
    complex concepts like LSTM, RNN, cross validation and hyper parameter tuning.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**Akshay Nautiyal** 是Quantinsti的定量分析师，致力于机器学习和金融的交汇处。QuantInsti是一个高端的算法和定量交易学院，提供[教师引导](https://www.quantinsti.com/epat)和[自学](https://quantra.quantinsti.com/courses)的学习项目。例如，有一个互动的[课程](https://quantra.quantinsti.com/learning-track/machine-learning-deep-learning-in-financial-markets)使用机器学习在金融市场，提供LSTM，RNN，交叉验证和超参数调整等复杂概念的实践培训。'
- en: Industry update
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**行业更新**'
