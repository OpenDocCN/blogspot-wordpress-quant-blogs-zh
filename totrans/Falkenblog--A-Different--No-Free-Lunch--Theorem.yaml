- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-05-12 21:10:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-12 21:10:36
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Falkenblog: A Different ''No Free Lunch'' Theorem'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Falkenblog: A Different ''No Free Lunch'' Theorem'
- en: 来源：[http://falkenblog.blogspot.com/2011/01/different-no-free-lunch-theorem.html#0001-01-01](http://falkenblog.blogspot.com/2011/01/different-no-free-lunch-theorem.html#0001-01-01)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[http://falkenblog.blogspot.com/2011/01/different-no-free-lunch-theorem.html#0001-01-01](http://falkenblog.blogspot.com/2011/01/different-no-free-lunch-theorem.html#0001-01-01)
- en: '[![](img/aa82f2b12772c022f165a0cd4eb6e330.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhX4V2tu_7F0SbxsbyB2X9fmv024R3yowBF-XKaEgBwj7leM8bnE-qwuGa3xjb8cmw-cjqGGrDUgco8bh-GMwVIYEeX35VfU6eFciiQ3M5XFkVHGT4wQvETp3p6yOE3tJUGjRK2A/s1600/Jeff_Hawkins.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/aa82f2b12772c022f165a0cd4eb6e330.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhX4V2tu_7F0SbxsbyB2X9fmv024R3yowBF-XKaEgBwj7leM8bnE-qwuGa3xjb8cmw-cjqGGrDUgco8bh-GMwVIYEeX35VfU6eFciiQ3M5XFkVHGT4wQvETp3p6yOE3tJUGjRK2A/s1600/Jeff_Hawkins.jpg)'
- en: Milton Friedman popularized the phrase 'there's no such thing as a free lunch'
    by using it as the title of a 1975 book, and it often appears in economics textbooks.
    It is a core idea in economics, but I was surprised to learn it comes up in AI.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 弗里德曼通过将其用作 1975 年一本书的标题而使短语“没有免费午餐”变得流行，并且经常出现在经济学教科书中。这是经济学的核心理念，但我惊讶地发现它也出现在人工智能中。
- en: My experience with neural nets is decidedly negative. They overfit and provide
    no intuition for modification. They are consistent estimators, that is, with enough
    observations they will find patterns, but financial data are notoriously correlated
    in such a way that we rarely have enough data to make these asymptotic properties
    relevant. That is, look at cross sectional equity returns. There are officially
    around 5000 stocks in the USA, so after a decade, you have about 12 million daily
    observations, which seems like a lot. Yet the really small 2000 stocks are still
    measured poorly in standard databases, as in the CRSP tapes a strong 'low price'
    effect still exists even after they supposedly rid the data of the delisting bias
    discovered by Shumway (delisted firms would often have 'N/A' for their final return,
    when in fact they would lose 40%). But then even with these 3000 stocks left,
    if you simply find something correlated with the internet bubble or the mortgage
    crisis, you will discover a high t-stat in explaining cross-sectional returns,
    and these patterns are
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我对神经网络的经验显然是负面的。它们会过度拟合，并且不提供修改的直觉。它们是一致的估计量，也就是说，有足够的观察数据，它们将找到模式，但是金融数据以这样的方式相关，以至于我们很少有足够的数据使这些渐近特性相关。也就是说，看看横截面股票回报。美国有大约
    5000 只股票，所以十年后，你大约有 1200 万个每天的观察数据，这似乎很多。但是即使是剩下的 2000 只股票，如果你简单地找到与互联网泡沫或抵押贷款危机相关的东西，你将发现在解释横截面回报时有一个高
    t 值，并且这些模式是
- en: sui generis
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: sui generis
- en: ', not fundamental characteristics of the data.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ，而不是数据的基本特征。
- en: '[![](img/ff12253a2a789ffa9a33a9aa083ebf60.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZkMw2a1obZWJJDU5u2eKz9Jo6YoDaFTmCH48OqiRVhB6wulEftHIpqsjHLYqLvIYXUOvpNyfW_bbvE1Gtwl_36vBTX8kP3IrclWzkt9ptGjHEvw4AS6TMIia9LpphXWGdYuT2rQ/s1600/onintellegince.png)[Jeff
    Hawkins](http://www.youtube.com/watch?v=G6CVj5IQkzk)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/ff12253a2a789ffa9a33a9aa083ebf60.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZkMw2a1obZWJJDU5u2eKz9Jo6YoDaFTmCH48OqiRVhB6wulEftHIpqsjHLYqLvIYXUOvpNyfW_bbvE1Gtwl_36vBTX8kP3IrclWzkt9ptGjHEvw4AS6TMIia9LpphXWGdYuT2rQ/s1600/onintellegince.png)[杰夫·霍金斯](http://www.youtube.com/watch?v=G6CVj5IQkzk)'
- en: is a successful computer programmer who is really interested in aping the brain
    to come up with more efficient algorithms. In his book
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 是一位成功的计算机程序员，对模仿大脑以提出更高效的算法非常感兴趣。在他的书籍
- en: '[On Intelligence](http://www.amazon.com/Intelligence-Jeff-Hawkins/dp/0805074562)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[《智慧之源》](http://www.amazon.com/Intelligence-Jeff-Hawkins/dp/0805074562)'
- en: ', he argues our neocortext is the essence of our intelligence. It is the wrinkly
    surface of mammalian brains that is about the size of a napkin folded up in your
    head. 1k square centimeters, 2-3mm thick, about 30 billion neurons. He calls this
    Heirarchical Temporal Memory because data is stored in heirarchies: we understand
    phonemes, words, phrases, ideas, each one a higher level understanding of the
    concepts below it.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ，他认为我们的新皮层是我们智慧的本质。这是哺乳动物大脑的皱褶表面，大约相当于头部折叠的餐巾大小。1k 平方厘米，2-3mm 厚，大约 300 亿个神经元。他称之为分层时间记忆，因为数据存储在层次结构中：我们理解音素、单词、短语、观念，每个都是对其下概念的更高层次理解。
- en: 'We experience the world through a sequence of patterns, and when we recall
    them we do so to predict. It''s an internal metric, not one based on behaviorism.
    Most patterns are spatial or temporal, in that a melody has has a a chord (set
    of frequencies), and the temporal relation of of those chords. When we comprehend
    a piece of music we are making predictions about what comes next, anticipating
    the next note. We recognize the song when our predictions are correct, and this
    generates an ''aha!'' moment, as when you see the dalmation in the picture of
    black and white spots. Hawkins argues that intelligence is essentially this, the
    prediction of these patterns: when we predict something well our neocortex strengthens
    these neural connections, when it doesn''t predict well the unsuccessful pattern
    disappears. The brain makes predictions down to what we perceive, and up to what
    we interpret. Intelligence is prediction.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一系列模式体验世界，当我们回忆它们时，我们是在预测。 这是一个内部的标准，而不是基于行为主义的标准。 大多数模式是空间或时间上的，一个旋律有一个和弦（一组频率），还有这些和弦的时间关系。
    当我们理解一首音乐时，我们在预测下一个出现的音符，预测下一个音符。 当我们的预测是正确的时候，我们识别出这首歌，这产生了一个“啊哈！”的时刻，就像你在黑白斑点图片中看到达尔马提亚犬。
    霍金斯认为智能本质上就是这样，预测这些模式：当我们预测得很好时，我们的新皮层加强这些神经连接，当他不能很好地预测时，不成功的模式就消失了。 大脑从我们看到的预测，到我们理解的预测。
    智能就是预测。
- en: It's an intriguing concept, and a rather stark contrast to two ideas from Alan
    Turing. The first, that the brain is just a big computer, with a bunch of AND-gates
    and OR-gates, just like logical circuits. Turing proved mathematically that with
    an infinitely long program, it can perform any definable set of operations in
    the universe. His second idea was that you could judge whether a computer is intelligence
    if you can't tell it's not human by asking it questions, and see if you could
    tell it was a computer.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的概念，和艾伦·图灵的两个想法形成了鲜明的对比。 第一，大脑只是一个大型计算机，有一堆与或门，就像逻辑电路一样。 图灵在数学上证明，用一个无限长的程序，可以在宇宙中执行任何可定义的操作。
    他的第二个想法是，如果你问一个问题，并且看到它是一个计算机，你能否判断一个计算机是否具有智能，并且看到你能否判断它是一个计算机。
- en: As mentioned, Hawkins thinks intelligence is not emulating the behavior of humans,
    but rather, predicting well at every level, mainly for the many little consistencies
    we see in how the things in the world relate spatially and temporally. Secondly,
    there is the problem that Turing's program is very inefficient. There is a '
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，霍金斯认为智能不是模仿人类的行为，而是预测每个级别，主要是因为我们在如何空间和时间上的关联中看到许多小一致性。 其次，还存在图灵程序非常低效的问题。
    还有一点是，’
- en: '[no free lunch](http://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[无免费午餐](http://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization)'
- en: ''' theorem in optimization programs: no learning algorithm can be better than
    all other learning algorithms for all problems. To be best for one problem you
    have to tune it to the problem at hand. A Turing machine makes no assumptions,
    but you pay a cost because it is not necessarily organized hierarchically or temporally,
    which turns out to be a good way of describing our world.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 优化程序中的定理：对于所有问题，没有学习算法可以优于所有其他学习算法。 要成为一个问题的最佳解决方案，你必须调整它以适应手头的问题。 图灵机不做出任何假设，但你要付出代价，因为它并不一定是按层次化或时间顺序组织的，而这其实是描述我们的世界的一种很好的方式。
- en: A general algorithm must be tweaked to incorporate the specific problem at hand.
    This mainly concerns the selection of inputs (not too many), and their transformation
    (sigmoidal transformation, putting raw data into stationary time series, etc.),
    but also the functional form. This 'outside the box' framing problem uses up a
    lot of degrees of freedom, but doesn't really show up anywhere because once done,
    any algorithm ignores them in their standard errors, as if this was the first
    and only way of apprehending the data. Specific algorithms contain a lot of specialized
    intuition or common sense, which is why really smart people are often clueless
    at modeling the real world.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通用算法必须调整以融合特定的问题。 这主要涉及选择输入（不要太多）及其转换（将原始数据转换为稳态时间序列等），以及功能形式。 这个“跳出框架”的问题使用了许多自由度，但实际上不会出现在任何地方，因为一旦完成，任何算法都会在其标准误差中忽略它们，就好像这是把握数据的第一种唯一方法。
- en: Hawkins' book is a good read, and he has several interesting videos on YouTube.
    I like the idea of the no free lunch, and adding structure in the way he mentions.
    Yet, I still think his ideas are too unstructured. He has had access to a lot
    of funding and has been working in this dimension for almost 10 years. He seemed
    to think his track was on the cusp (couple years) of finding some really better
    artificial intelligence algorithm back in 2005 or so. I haven't see those. His
    signature achievement, the creation of the
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 霍金斯的书写得很好，他在YouTube上有几个有趣的视频。我喜欢没有免费午餐的想法，以及他所提到的增加结构的方式。然而，我仍然认为他的想法过于不结构化。他一直有大量资金，并且在这个领域已经工作了将近10年。他似乎认为他的轨迹在2005年左右就处于找到一些真正更好的人工智能算法的边缘。我还没有见过那些。他的标志性成就，
- en: '[Graffiti software](http://en.wikipedia.org/wiki/Graffiti_(Palm_OS))'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[涂鸦软件](http://zh.wikipedia.org/wiki/Graffiti_(Palm_OS))'
- en: to recognize letters in things like the Palm Pilot, seems like your typical
    useful, highly specialized algorithm. I suppose some neural net modified in this
    'temporal hierarchical' structure is better than one with merely layers of hidden
    nodes, but that still leave a lot undefined in the step where you choose what
    to look and what functional forms it should constrain itself to. Robot maids,
    machines with general intelligence, are still science fiction. Instead, we have
    great machines for applying specific logic, which invariably is specialized in
    such a way to make any big optimization idea rather uninteresting. The 'No Free
    Lunch' theorem Hawkins mentions seems to counter his big idea.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 识别诸如Palm Pilot之类的东西中的字母，似乎是您典型的有用的，高度专业化的算法。我想，一些修改为这种“时间分层”结构的神经网络比仅具有隐藏节点层的神经网络更好，但在选择要查看的内容和应该约束自身的功能形式的步骤中，仍然有很多未定义的内容。机器女仆，具有通用智能的机器，仍然是科幻。相反，我们有很多优秀的机器来应用特定逻辑，这种逻辑无一例外地在某种程度上是专门化的，以至于使得任何大型优化想法都变得不那么有趣。霍金斯提到的“没有免费午餐”定理似乎与他的重要观点相矛盾。
