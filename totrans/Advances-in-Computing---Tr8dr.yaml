- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-18 15:36:03'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Advances in Computing | Tr8dr
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://tr8dr.wordpress.com/2009/12/15/advances-in-computing/#0001-01-01](https://tr8dr.wordpress.com/2009/12/15/advances-in-computing/#0001-01-01)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: December 15, 2009 · 2:21 pm
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Have been watching advances in computing power for some time, since university
    really.   I did research in parallel algorithms and architecture for my first
    position at the university and later applied practically on Wall Street.   In
    those days super-expensive machines like the Intel Hypercube, Paragon, and many
    other architectures were the backbone of the HPC community.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'HPC (High Performance Computing) roughly breaks down into 4 categories:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Big iron supercomputers (MIMD generally)
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distributed computing (these days advertised as Cloud Computing)
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The emerging SIMD GPU based solutions
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Quantum Computing (not really here yet for the mainstream)
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the machine learning and optimisation world there are massive problems, some
    of which are not computable on von-neumann architectures, as their runtime would
    be astronomical.    An (absurd) example of such a problem would be to simulate
    a large number monkeys typing on typewriters, stopping when one produces the works
    of Shakespeare.   The number of monkeys required to produce such a work on average
    in astronomical.     This seems like an absurd problem, but is comparable to the
    GP / GA approach.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Then of course there are numerous problems with high dimensionality and/or with
    polynomial order complexity.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '**Supercomputing on the Cheap** The [FASTRA](http://fastra.ua.ac.be/en/index.html)
    team at the University of Antwerp has put together an inexpensive multi-teraflop
    machine with 7 gaming cards.  Check out their video.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately the “easy” part of these sort of solutions is the hardware.  The
    problem is the (often) great expense to develop one’s models in a SIMD framework,
    so can be applied to for the GPU architecture.    Although there is now standardization
    on the low-level C-variant used to program GPUs, there are significant differences
    between different models of GPUs, that even if you manage to write a correct SIMD
    program, may have to rearrange for a specific GPU implementation.   (I guess this
    is not all that different from my experiences with big-iron parallel architectures
    of the past).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: One could have a team devoted to parallelization, tuning, and retuning / reworking
    for the new GPUs that are out periodically.   Very time consuming!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: For my work, the problems that would map well are particle filters and monte-carlo
    based models, each of which have obvious fine-grained parallel operations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantum Computing** The other notable announcement this week was Google’s
    use of [quantum computing](http://googleresearch.blogspot.com/2009/12/machine-learning-with-quantum.html)
    to solve pattern recognition problems.   I have not done the leg-work to fully
    understand the algorithms in quantum computing, but broadly it seems to be a matter
    of framing one’s problems statistically as path integration problems (i.e., expectations),
    where quantum computing allows the paths to be explored simultaneously.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**量子计算** 本周另一个值得注意的公告是谷歌利用[量子计算](http://googleresearch.blogspot.com/2009/12/machine-learning-with-quantum.html)来解决模式识别问题。
    我还没有完全理解量子计算中的算法，但总体来说，它似乎是将问题统计地框定为路径积分问题（即期望），其中量子计算允许同时探索路径。'
