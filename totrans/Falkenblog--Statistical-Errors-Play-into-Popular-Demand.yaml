- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-12 22:20:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'Falkenblog: Statistical Errors Play into Popular Demand'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[http://falkenblog.blogspot.com/2009/02/statistical-errors-play-into-popular.html#0001-01-01](http://falkenblog.blogspot.com/2009/02/statistical-errors-play-into-popular.html#0001-01-01)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[A case for Due Diligence in Policy Formation:](http://www.fraserinstitute.org/commerce.web/product_files/CaseforDueDiligence_Cda.pdf)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: In 1993, a team of researchers led by D.W. Dockery and C.A. Pope published a
    study in he New England Journal of Medicine supposedly showing a statistically
    significant correlation between atmospheric fine particulate levels and premature
    mortality in six US cities.
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The audit of the HSC data reported no material problems in replicating the original
    results, though there were a few coding errors. However, their sensitivity analysis
    showed the risk originally attributed to articles became insignificant when sulphur
    dioxide was included in the model, and the estimated health effects differed by
    educational attainment and region, weakening the plausibility of the original
    findings. The HEI also found that there were simultaneous effects of different
    pollutants that needed to be included in the analysis to obtain more accurate
    results.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This is always the case, as empirical errors are mainly parochial issues involved
    the data, as opposed to whether one used 2 or 3-stage least squares. The authors
    go over several high-impact studies that were initially well-received and now
    seen as impossibly flawed: The Boston Fed discrimination study, the Global Warming
    Hockey stick graph, the Freakonomics Abortion and Crime study, the Bellesiles
    documentation that early Americans rarely owned guns, and others.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Doing complex problem sets with nuances that are really abstract helps you know
    the basics, yet at the end of it all the useful tools of analysis are all covered
    in undergraduate statistics. It takes years of study to really understand it,
    as I remember not really understanding basic probability and statistics until
    I TA'd the course three times in graduate school. They key is controlling for
    relevant omitted variables, knowing whether the relevant variability is over time
    or within a time period, how the errors are distributed, and other prosaic issues.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Academics are really good at proofs, refinements of cutting edge applications,
    but these are generally pretty irrelevant to any practical issue. The important
    insights in data come from understanding the data. In every case, the problem
    studies had one key commonality:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: they were results people wanted to believe
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '. When people publish results consistent with intuition, especially when it
    is done in an academic way by making abstruse second order adjustments to coefficients
    and standard errors, this gives any piece sufficient superficial rigor so that
    you can then say to your critics: "if you do not understand instrumental variables
    (say), then you cannot criticize this study!" People love using these kinds of
    papers to buttress their pet policies, and thus they become popular.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Consider the
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[study](http://research.chicagogsb.edu/IGM/docs/ZingalesCulturGenderMath.pdf)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: put out by four economists led by Paola Sapienza and Luigi Zingales in economics
    that supposedly proved the sex gap in mathematical abilities disappears when everyone
    embraces women's emancipation. In other words, The Man is at fault, as usual,
    for observed human biodiversity, a prized academic result (the Dean may show up
    at your seminar!). An anonymous blogger,
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[La Grifffe du Lion](http://www.lagriffedulion.f2s.com/math2.htm)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: ', demolishes this and related studies by adjusting for the following facts:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: the math gap mainly occurs only after the onset of puberty
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: simple proficiency tests do not measure ability relevant to explaining the proportion
    of engineers and top scientists, who are all highly proficient at mathematics
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the means are not as important as the relative variances of the distributions
    when comparing the proportion of groups in extremums
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: higher IQ countries implies the level of 'proficiency' is less of an extreme
    tail cutoff, which lower group differences.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These corrections are not purely statistical, rather, they come from understanding
    the issue (eg, that gender differences appear only after hormones start diverging).
    It comes from understanding the phenomenon in question, as opposed to some abstract
    second-order statistical tweak.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: The best way to succeed in academics is to assess what the zeitgeist currently
    wants documented, and give it to them in a long article with lots of abstruse
    mathematics. By the time someone says, "you assumed unobserved credit scores of
    the applications is uncorrelated with race", well, by then you have tenure and
    can call such criticisms hair splitting (though, conspicuously, you never publish
    on that subject again). Indeed, Liebowitz's rebuttal of Munnell's Boston Fed study
    is not great econometrics, and so Munnell was published in the AER, Liebowitz
    in the Journal of Obscure Economics, yet Liebowitz was right and Munnel wrong.
    But the result of Munnell was something many people, including academics, wanted
    to be true, so they read the conclusion (we can costlessly help poor people!),
    focused on second-order statistical adjustments but ignored bigger issues like
    omitted variables, noisy data, and the fact that default rates were, if anything,
    higher on the group than supposedly is held to a higher credit standard. While
    some might have noticed the increase in housing prices post 2000, no one was on
    the warpath saying we need to make current lending criteria more stringent prior
    to 2007\. The cause of this mortgage bubble was something very few people wanted
    to tackle.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要在学术方面取得成功的最佳方法是评估当前了解的时代精神，然后用一篇长篇文章和许多晦涩的数学知识展示给他们。当有人说：“你假设未观察到的信用评分与种族无关”时，此时你已经终身教职资格，并且可以称此类批评是爱挑刺（尽管显眼的是，你再也不在这个主题上发表文章）。事实上，利博维茨对蒙内尔的波士顿联邦储备银行研究的反驳并不是很好的计量经济学，所以蒙内尔在AER上发表，而利博维茨在《显微经济学杂志》上发表，然而利博维茨是正确的而蒙内尔是错误的。但蒙内尔的结果是许多人，包括学者，希望成真的，所以他们读了结论（我们可以无成本地帮助贫困人口！），关注了二阶统计调整，但忽略了更大的问题，比如省略变量，嘈杂数据，以及默认率实际上比所谓的高信用标准群体更高。虽然一些人可能注意到了2000年后房价的增长，但在2007年之前没有人公开表示我们需要加强当前的放贷标准。这次抵押贷款泡沫的原因是很少有人愿意处理的问题。
- en: Thus, the problem in the mortgage debacle is not some specific statistical test,
    an obscure Value-at-Risk assumption buried in a footnote of Jorion's text, but
    merely an assumption anyone could understand (housing prices do go down, no money
    down is a very dangerous 'innovation'), yet no one cared about. Investment banks,
    academics, and regulators considered this scenario irrelevant for a variety of
    reasons, but most importantly because it was intellectually defensible (mortgage
    loss rates had not increased since underwriting standards started deteriorating
    in the early 1990's due to regulatory pressure), and it was something they wanted
    to believe (if true, it was generated a lot of money and votes for everyone involved).
    We can now go back and find all the technical errors by Moody's or the Boston
    Fed study, but anyone who thinks that moving from a 10-day VaR to an annual horizon
    is missing the deeper point, that the assumptions most dangerous are the one's
    that are changing due to some new big trend--tax shelter REIT's in the 1980's,
    new economy companies in the late 1990's--and people applying the old standard
    find themselves marginalized in the boom, and they become powerless if not jobless
    at the peak.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在抵押债务危机中的问题并不是某个具体的统计测试，而是包含在约里昂文本的一个晦涩的Value-at-Risk假设的脚注中，而只是任何人都能理解的假设（房价会下跌，无首付是一个非常危险的“创新”），然而没有人关心。投资银行，学者和监管者认为对这种情况的关注出于各种原因是无关紧要，但最重要的是因为这是可以得到理性辩护的（自从由于监管压力，自1990年代初贷款标准开始恶化以来，抵押贷款损失率并没有增加），并且他们希望这是真的（如果是真的，这将为所有涉及方带来大量的金钱和选票）。现在我们可以回过头来找到穆迪公司或波士顿联邦储备银行研究中的全部技术错误，但任何认为从10天的VaR移动到年度视角的人都忽略了更深层次的问题，即最危险的假设是因某种新的大趋势而改变的那些假设--比如20世纪80年代的减税避税REIT公司，20世纪90年代末的新经济公司--并且套用旧标准的人会在繁荣时期感到被边缘化，并且在巅峰时期变得无权力，甚至失业。
