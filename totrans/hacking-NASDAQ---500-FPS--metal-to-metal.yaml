- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-13 00:05:49'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-13 00:05:49
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'hacking NASDAQ @ 500 FPS: metal to metal'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NASDAQ 黑客 @ 500 FPS：金属对金属
- en: 来源：[http://hackingnasdaq.blogspot.com/2010/02/metal-to-metal.html#0001-01-01](http://hackingnasdaq.blogspot.com/2010/02/metal-to-metal.html#0001-01-01)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[http://hackingnasdaq.blogspot.com/2010/02/metal-to-metal.html#0001-01-01](http://hackingnasdaq.blogspot.com/2010/02/metal-to-metal.html#0001-01-01)
- en: Been busy on some other stuff the last week or so, and time to get back to this.
    Our last investigation looked into how the stock linux scheduler is responsible
    for a fair chunk of the network latency we`ve been seeing. Thus the logical question
    is, what happens if we ditch it all and write to the metal?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 上周忙于其他事情，现在是时候回到这个问题了。我们上次的调查看了一下，我们看到的网络延迟的相当大一部分是由于 stock linux scheduler
    引起的。因此，一个很自然的问题是，如果我们放弃一切直接与底层硬件交互会发生什么呢？
- en: Intel hardware is great for this, the hw docs are excellent and reference sauce/linux
    drivers are nice and easy to follow. Whats the setup? pretty simple, wrote a linux
    driver which maps BAR0 (register space) and BAR1(flash memory) directly into user
    space, then allocate and map into the users address space 4 dma buffers, Rx Descriptors,
    Rx Buffer, Tx Desciptors, Tx Buffer. Which is all we need to write our own PHY/MAC/Ether/IPV4
    + ARP/ICMP/UDP/TCP stack. Its a straight forward process, with the only hairy
    part being the TCP stack. Got everything up and running except TCP in a few days
    as NIC hardware is pretty dam simple and connection-less protocols are nice and
    easy. Certainly not production ready but enough to get some interesting results.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Intel 硬件非常适合这个用途，硬件文档非常出色，参考 sauce/linux 驱动非常易于跟踪。设置是什么？非常简单，编写一个将 BAR0（寄存器空间）和
    BAR1（闪存）直接映射到用户空间的 linux 驱动程序，然后为用户空间分配和映射 4 个 DMA 缓冲区，Rx 描述符，Rx 缓冲区，Tx 描述符，Tx
    缓冲区。这就是我们编写自己的 PHY/MAC/Ether/IPV4 + ARP/ICMP/UDP/TCP 栈所需的一切。这是一个直截了当的过程，唯一棘手的部分是
    TCP 栈。除了 TCP，几天内就把所有东西都运行起来了，因为 NIC 硬件非常简单，无连接协议非常轻松。当然还不完全成熟，但足以获得一些有趣的结果。
- en: Lets start with round trip latency. Its the same 2 machines, we`re using ICMP
    64byte payload for this test - note the time scales are different.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从往返延迟开始。这两台机器相同，我们使用 ICMP 64 字节负载进行此测试 - 请注意时间刻度不同。
- en: 64B ICMP round trip (metal)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 64B ICMP 往返（金属）
- en: '[![](img/6b4abc081a486ad6788e9e0f92bfa5c9.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4i9JAr-1CZa14-b3I57IvbVOQj5AoaPHi8GAgcs1qBed0L8QdM8rCY-8_yS71nAcqnk9KiION7O1thCJ9G6c8LmKlhtIHd6YAwXDi5zB44otDQ9xc9y87UeN9zX9_jijCCgo9JGRnPA/s1600-h/tcp_round_default_poll.png)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/6b4abc081a486ad6788e9e0f92bfa5c9.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4i9JAr-1CZa14-b3I57IvbVOQj5AoaPHi8GAgcs1qBed0L8QdM8rCY-8_yS71nAcqnk9KiION7O1thCJ9G6c8LmKlhtIHd6YAwXDi5zB44otDQ9xc9y87UeN9zX9_jijCCgo9JGRnPA/s1600-h/tcp_round_default_poll.png)'
- en: 128B TCP round trip (polling)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 128B TCP 往返（轮询）
- en: Our metal gets on average around 19,000ns for the total round trip, so we can
    average and say 9,500ns on each side, but it isnt really 50/50 as we shall see.
    Still while not apples-apples comparison, compared to our best result so far,
    polling TCP, we are about 15,000ns or so faster than the linux stack, almost x2
    nice!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的金属在总体往返时间上平均大约为19,000ns，因此我们可以平均说每一边是9,500ns，但实际上并非一半一半，我们会看到。尽管不是苹果和苹果的比较，但与我们迄今为止的最佳结果相比，轮询
    TCP，我们比 linux 栈快了大约 15,000ns，几乎是 x2 多！
- en: So where does that 19,000ns go? Its quite surprising and actually rather depressing
    how shite x86 IO architecture is. First the results
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这 19,000ns 去哪了？令人惊讶的是，在 x86 IO 架构上的表现实际上令人相当沮丧。首先是结果。
- en: Machine A - latency 1 32b register read
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器A - 延迟 1 32位寄存器读取
- en: Machine B - latency 1 32b register read
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 机器B - 延迟 1 32位寄存器读取
- en: '... and as you can see, Machine A takes about 800ns to read 1 register, and
    Machine B is 2,200ns for *one* read. Why are the 2 machines so different? as discussed
    before Machine B has its NIC on the PCIe bus, while Machine A is wired directly
    to the south bridge. Thus while machine B is a fancy pants latest and greatest
    nehalem and Machine A an old dog, 2007(ish) Xeon, the old dog wins and really
    drives home the point how important physical hardware topology is at this performance
    level.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '... 如你所见，机器A大约需要800ns读取1个寄存器，而机器B需要2,200ns读取*一个*。为什么这两台机器如此不同？正如之前所讨论的，机器B的
    NIC 在 PCIe 总线上，而机器A直接连接到南桥。因此，尽管机器B是最新款的 nehalem，而机器A是2007年左右的旧款 Xeon，但旧款 Xeon
    知道如何取胜，真正印证了在这个性能水平上物理硬件拓扑是多么重要。'
- en: Your regular every day drivers will be interrupt (ish.. NAPI polling) driven,
    starting with an Rx isr that reads the interrupt status register to work out wtf
    is going on, then reads the Rx descriptor head register, to see how much/where
    the new packets are. If we summarize using fancy pants machine B.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你日常使用的驱动程序将是中断（ish.. NAPI轮询）驱动，从一个Rx isr开始，读取中断状态寄存器以确定发生了什么，然后读取Rx描述符头寄存器，查看新数据包的数量和位置.
    如果我们使用“花哨的机器B”来进行总结。
- en: 'MAC -> CPU isr latency : 2,200 / 2 = 1,100ns'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: MAC -> CPU isr延迟：2,200 / 2 = 1,100ns
- en: 'CPU isr status register read: 2,200ns'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CPU isr状态寄存器读取：2,200ns
- en: 'CPU rx desc head register read: 2,200ns'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: CPU rx描述符头寄存器读取：2,200ns
- en: Putting latency at around 5,500ns, that`s *before* protocol processing, and
    we haven't even looked at the Rx descriptor or Rx buffer data yet!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 将延迟设定在大约5,500ns，这是 *协议处理之前*，而我们甚至还没有查看Rx描述符或Rx缓冲区数据！
- en: Machine A Rx Desciptor load(DDR)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 机器A Rx描述符加载（DDR）
- en: '[![](img/7891a8dc097760742431237195d6af20.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYnBCnHQD5X6JRNpETpUx2zyIfs1EMMLNopFfT9b-GnttJfCBu9PDvcpJE8knulKN62M3o2dY17MMfIAwdNPB-FUEkFyPNTMAS4BGjRRSc06JID4W7j9dFA7mxburKngrQqOGFNitb_A/s1600-h/rxdesc_read_b.png)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/7891a8dc097760742431237195d6af20.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYnBCnHQD5X6JRNpETpUx2zyIfs1EMMLNopFfT9b-GnttJfCBu9PDvcpJE8knulKN62M3o2dY17MMfIAwdNPB-FUEkFyPNTMAS4BGjRRSc06JID4W7j9dFA7mxburKngrQqOGFNitb_A/s1600-h/rxdesc_read_b.png)'
- en: Machine B Rx Desciptor load(DDR)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 机器B Rx描述符加载（DDR）
- en: Rx descriptors are in DDR and thankfully can be cached, however the problem
    is when a device writes into DDR, i beleive, and numbers seem to agree, it invalidates
    the line(s) from all levels of the cache, which kind of sucks. I kind of expected
    nehalem would just invalidate the l1/l2, and update the L3, but seems its not
    that smart Thus in the plots above we see the cost of a l1/l2/l3 miss, so around
    100ns/270 cycles on Machine A, and 75ns/200cycles on Machine B. With Machine B
    faster due to the memory controller being on the CPU instead of the north bridge
    - one less bus to travel.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Rx描述符位于DDR中，令人欣慰的是可以对其进行缓存，但问题在于当设备写入DDR时，我相信（而且数字看起来也是一致的），它会使所有级别的缓存行无效，这有些糟糕。我原本以为尼哈勒姆会只使l1/l2无效，并更新L3，但看起来它没那么聪明。因此在上面的图表中，我们可以看到在机器A上缓存未命中的成本约为100ns/270个周期，在机器B上约为75ns/200个周期。由于机器B更快，因为内存控制器位于CPU上而不是北桥上-少了一个总线要走。
- en: In summary
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：
- en: 'Wire -> PHY : ?ns'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '电线 -> PHY : ?ns'
- en: 'PHY -> MAC : ?ns'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 'PHY -> MAC : ?ns'
- en: 'MAC -> CPU isr latency : 2,200 / 2 = 1,100ns'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: MAC -> CPU isr延迟：2,200 / 2 = 1,100ns
- en: 'CPU isr status register read : 2,200'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: CPU isr状态寄存器读取：2,200
- en: 'CPU rx desc head read : 2,200'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: CPU rx描述符头读取：2,200
- en: 'CPU rx desc read : 75ns'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: CPU rx描述符读取：75ns
- en: 'CPU rx buffer read : 75ns'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: CPU rx缓冲区读取：75ns
- en: 'Total: 5,650ns or 15,000 cycles already! Obviously we can pipeline some of
    this but you wont find that and other optimizations in the standard drivers. Actually
    on x86, not certain you can issue multiple non cached reads, that are not guarded/serialized
    e.g. pipelined. Moral of the story, its a challenge to get single digit microsecond
    latency to turn a trade around (Rx -> Tx), measured from the wire, using standard
    PC server hardware.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 总计：5,650ns 或 15,000个周期！显然我们可以对其中一些进行流水线处理，但是你在标准驱动程序中不会找到这些和其他优化。实际上，在x86上，不能确定你是否能发出多个非缓存读取，而这些读取没有受到保护/序列化，例如流水线处理。故事的寓意是，利用标准PC服务器硬件让单个微秒延迟转变（Rx
    -> Tx）是一项挑战，从电线上测量，。
