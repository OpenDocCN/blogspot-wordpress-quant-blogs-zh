- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-12 18:06:35'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive Percentile Position Sizing | CSSA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://cssanalytics.wordpress.com/2012/03/07/adaptive-percentile-position-sizing/#0001-01-01](https://cssanalytics.wordpress.com/2012/03/07/adaptive-percentile-position-sizing/#0001-01-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The most common method of position sizing uses a fixed percentage risk divided
    by volatility to dictate the fraction of the account size to invest. In generic
    terms this is:'
  prefs: []
  type: TYPE_NORMAL
- en: P= F/V
  prefs: []
  type: TYPE_NORMAL
- en: where F= typically 1% and
  prefs: []
  type: TYPE_NORMAL
- en: V= daily volatility (non-annualized)
  prefs: []
  type: TYPE_NORMAL
- en: P= portfolio position size
  prefs: []
  type: TYPE_NORMAL
- en: 'example: if V=2% and F=1% then the P= 50%'
  prefs: []
  type: TYPE_NORMAL
- en: Standard deviation represents a generic measurement of dispersion that is bi-directional
    and does not favor any portion of the distribution over the other. One standard
    deviation contains 68% of the distribution of returns, which in percentile terms
    (assuming normality) would equate the 84th and 16th percentiles. But what if certain
    percentiles contained more information than others for position sizing? In a past
    post (D-Var Position Sizing) I used the 5th percentile for position-sizing to
    capture tail risk. The logical notion was that to compound wealth, we want to
    avoid large losses and thus should size based on our empirical observation of
    extreme losses. But the 5th percentile is rather arbitrary–why not the worst loss
    (0) or the 2nd percentile? Perhaps the 25th or even the 65th percentile contain
    significant value. How do we capture the portions of the distribution that contain
    the most value or even more importantly, how do we combine this information?
  prefs: []
  type: TYPE_NORMAL
- en: 'What we need is an adaptive approach that considers the value of using different
    portions of the distribution simultaneously that may also have systematic differences
    in magnitude- for example the 2nd percentile would have a lower P than the 25th
    percentile. Here is a method that addresses these issues in a fairly compact manner:'
  prefs: []
  type: TYPE_NORMAL
- en: 1) find the percentile values using 5% increments from 0 to 1 using a trailing
    lookback of say 60-days and compute a column array of these values for some index-
    say the S&P500/SPY for at least 1000 bars
  prefs: []
  type: TYPE_NORMAL
- en: 2) compute P using a 1% as a default target and create a column array of P for
    each percentile interval
  prefs: []
  type: TYPE_NORMAL
- en: 3) releverage the percentile values to a fixed target–say 100% so that they
    have the same scale
  prefs: []
  type: TYPE_NORMAL
- en: 4) each day you would compute the average P over some lookback (60-252 days)
    and take 100%/average P and re-scale
  prefs: []
  type: TYPE_NORMAL
- en: 5) the current value so that all percentiles have the same average P
  prefs: []
  type: TYPE_NORMAL
- en: 6) compute equity curves that position size on the underlying index using P
    for each percentile interval
  prefs: []
  type: TYPE_NORMAL
- en: 7) using a solver or mean-variance optimization, create a set of possible weights
    for each percentile interval that best maximizes portfolio sharpe or MAR (calmar
    ratio, or return/max dd). use a lookback of say 2-3 years
  prefs: []
  type: TYPE_NORMAL
- en: 8) the weights times the P for each percentile interval create a weighted P
    which would be the adaptive percentile position size
  prefs: []
  type: TYPE_NORMAL
