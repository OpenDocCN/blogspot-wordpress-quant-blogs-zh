<!--yml
category: 未分类
date: 2024-05-18 13:59:54
-->

# Machine learning is for closers – Quantum Financier

> 来源：[https://quantumfinancier.wordpress.com/2017/12/23/machine-learning-is-for-closers/#0001-01-01](https://quantumfinancier.wordpress.com/2017/12/23/machine-learning-is-for-closers/#0001-01-01)

[Put that machine learning tutorial down. Machine learning is for closers only.](https://www.youtube.com/watch?v=QMFwFgG9NE8)

As some of you that were around back in the early of this blog may know, I always held high hopes for the application of machine learning (ml) to generate trading edges. I think like many people first coming across machine learning the promises of being able to feed raw data in some algorithm you don’t really understand to conjure profitable trading ideas seemingly out of thin air is very appealing. There is only one problem with that; it doesn’t work like that!

Much to my chagrin now, a lot (and I mean *a lot*) of what this blog is known for is exactly his type of silly applications of ml. With this post, I hope to share some of the mistakes I  made and lessons I learned trying to successfully make ml work for me that haven’t made it on the blog due to my abysmal posting frequency. Here they are, in no particular order:

*Understanding the algorithm you are using is important.*

It is almost too easy to use ml these days. Multiple times I would read a paper forecasting the markets using some obscure algorithm and would be able to, through proper application of google-fu, find open-sourced code that I could instantly try out with my  data. This is both good and bad; on the one hand it is nice to be able to use the cutting edge of research easily but of the other, should you really give something you don’t understand very well access to your trading account? Many of my dollars were mercilessly lit on fire walking that path. I don’t think you need to master the algorithm to be able to apply it successfully but understand it enough to be able to explain why it might be successful with your specific problem is a good start.

*Simple, not easy.*

One of my worst flaws as a trader is that I am relentlessly attracted to complex ideas. I dream about building out complex models able solve the market puzzle raking in billions of dollars a la RenTec. Meanwhile, back in the ruthless reality of the trading life, just about all the money I make trading comes from thoughtful application of simple concepts I understand well to generate meaningful trading edges. That however, does not mean that it needs has to be easy. For instance, there are multiple reasons why a properly applied ml algorithm might outperform say ordinary least-squares regression in certain cases. The trick is to figure out if the problem you are currently trying to solve is one of those. Related to the point above, understanding a ml technique allows you to have a better idea beforehand and saves you time.

*Feature engineering is often more important than the algorithm you choose.*

I cannot emphasize this point enough. A lot of the older posts on this blog are quite bad in that respect. Most of them use the spray-and-pray approach, that is to say put a bunch of technical indicators as features, cry .fit()!, and let slip the dogs of war as data-scientist Mark Antony would say. As you can imagine it is quite difficult to actually make that work and a lot of the nice equity curves generated by these signals don’t really hold up
out-of-sample. Not a particularly efficient way to go about it. Generating good features is the trader’s opportunity to leverage their market knowledge and add value to the process. Think of it as getting the algorithm to augment a trader’s ability, not replacing it altogether.

*Ensembles > single model.*

Classical finance theory teaches us that diversification through combining multiple uncorrelated bets is the only free-lunch left out there. In my experience, combining models is quite superior to trying to find the one model to rule them all.

*Model predictions can themselves be features.*

Model-stacking might seem counter-intuitive at first but there were many Kaggle competition winners that built very good models based on that concept. The idea is simple, use the predictions of ml models as features for a meta-model to generate the final output for the model.

I’ll conclude this non-exhaustive list by saying that the best results I have had come from using genetic programming to find a lot of simple edge that by themselves wouldn’t make great systems but when thoughtfully combined create a profitable platform. I will discuss the approach in forthcoming posts.