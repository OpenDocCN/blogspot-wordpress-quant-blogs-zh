- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-13 00:07:49'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'hacking NASDAQ @ 500 FPS: udp latency baseline'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[http://hackingnasdaq.blogspot.com/2010/01/space-time-continum.html#0001-01-01](http://hackingnasdaq.blogspot.com/2010/01/space-time-continum.html#0001-01-01)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now that the NIC`s are sorted, we can look into the round trip latency numbers.
    The guesstimate based on the previous post was around 3450ns, or 3.5us which is
    ... err.. "slightly" off..
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: First up we`ll go flat out so A sends as many packets to B, and B tries to relay
    them back to A. When A gets the packet, its got a send time stamp, so we can log
    the time delta into a histogram(below)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Sending at full rate
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Yes, that{s 200
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '**microsecconds**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'at the end, just a bit longer than 3500ns. What a mess.. Simplifying it down
    to a serial round trip latency histogram, translates to:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 1) A Send
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 2) B recv
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 3) B Send back to A
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 4 A Recv
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 5) A Next message
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: So the system is on a minimum work loading thus, should get the minimum round
    trip latency. We get the following graph(below) which is alot cleaner, even if
    highly disturbing
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Serial Round Trip
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it clocked in around 100,000ns ! that`s 96550ns longer than
    expected, and with a razor thin spread.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: So whats going on? For one a razor thin profile centered almost exactly on 100,000ns
    is extremely suspicious of a timer at work. With a little bit of digging, lo and
    behold there is, called Interrupt Coalescing - in the Intel Network card drivers
    and generally part of the NAPI. Its purpose is to reduce CPU load by batching
    interrupts into groups, so there's only 1 interrupt per say 32 packets, thus all
    32 packets *may* be processed in that single interrupt handler. Which is fine
    and great, improves network throughput and a good general all-purpose solution
    but is killing our low latency system.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: On page 19 on Intels "Interrupt Moderation Using GbE Controllers" manual there`s
    a fantastic graph.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: If you check the area circled, its the default parameter setting for intel drivers
    and looks... around 50,000ns which conveniently (round trip) adds up to what were
    seeing in our histogram 100,000ns. Thus lets mess with the parameter. First up
    is changing Machine B to InterruptThrottleRate=8000 (intels old default)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Machine B InterruptThrottleRate=8000
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: And great, our latency number moves (in the wrong direction) but none the less
    a very direct correlation. Thus lets disable all throttling on Machine B
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Machine B Interrupt Throttle Rate = OFF
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: And bang, we just reduced the latency by half! Not bad for a one liner. The
    Spikes are still extremely tall and thin, suggesting theres still a timer element
    in there. Next up, disable throttling for Machine A too.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Machine A & B Interrupt Throttle Rate = OFF
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: And finally we have somthing that has a fairly small spread, but not so thin
    that it looks like a timer, and must be close to our baseline round trip number.
    App -> Lib -> Kernel -> NIC -> Wire -> Switch -> Wire -> NIC -> Kernel -> Lib
    -> Bapp and back.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Machine A & B, Interrupt Throttling Rate = OFF
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 机器A和B，中断节流速率=关闭
- en: And a close up of what kind of latency we`re getting, calling it at 40,000ns
    round trip which ... is alot. Yet from the previous experiments 3500ns of that
    is in the throughput, so where did the rest go?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，我们得到了一种延迟的近距离测试，来回传输时间为40,000ns，这...太多了。然而根据之前的实验，其中有3500ns是在吞吐量中的，所剩下的时间去哪了呢？
