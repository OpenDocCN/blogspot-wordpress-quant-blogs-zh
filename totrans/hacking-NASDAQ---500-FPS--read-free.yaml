- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-13 00:05:20'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'hacking NASDAQ @ 500 FPS: read free'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[http://hackingnasdaq.blogspot.com/2010/02/read-free.html#0001-01-01](http://hackingnasdaq.blogspot.com/2010/02/read-free.html#0001-01-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If we profile the Rx side, its basically the cost of register read + some cold
    ddr fetch + a bit of fluffing around, thus the next logical step is to ditch all
    register reads. Its quite easy as the MAC will dma descriptor status and the contents
    directly into DDR thus... lets just poll the Rx Descriptor Status instead of reading
    the Rx head register.
  prefs: []
  type: TYPE_NORMAL
- en: 128B TCP round trip no register reads
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/656aac2e0d7b1b90a14eb16726a516b3.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUFOVl2tACzkD5I_Z1A6JHXPkkxA8zKRBRrpsOqi8xuDNTTxx-ZPGxXaUcszvqIb86QGBXE0Mxe2y7Ic_a6A5_fL_rraRnr35Ufm582zewNXBGTaKE1N8LEroU8hWNJ6CKZj6Ruh-uKQ/s1600-h/round_tcp_metal.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 128B TCP round trip register reads
  prefs: []
  type: TYPE_NORMAL
- en: 128B TCP round trip mental -> linux metal (with reg reads)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see we`re now a respectable 4-5,000ns faster than the linux stack!
    However care must be taken with this register free approach as im not certain
    what the memory ordering rules are here, e.g. if PCIe/SB writes are committed
    in-order or can be scheduled out of order. Its highly unlikely that by the time
    you`ve read the Rx Desc Status the payload data has not reached DDR. However,
    if you`ve got billions of dollars running through it you *really* want to be certain.
  prefs: []
  type: TYPE_NORMAL
- en: Finally the bulk of the speedup is from Machine B`s long ass register read times,
    thus the full NIC to NIC time on machine B for a 128B TCP packet is...
  prefs: []
  type: TYPE_NORMAL
- en: Machine B, NIC(CPU Rx) -> NIC(CPU Tx) latency
  prefs: []
  type: TYPE_NORMAL
- en: '... around 410ns and thats with SW checksums. To put that in perspective, my
    shitty FIX Fast(Arcabook compact) SW decoder runs around 140ns/packet assuming
    its hot in the cache. So we can clearly speed up TCP loopback code, but whats
    the point when hw latency is the bottleneck?'
  prefs: []
  type: TYPE_NORMAL
- en: Obviously the wire-wire time is a few microsecconds.. e.g. its going to take
    1,000+ ns just for the MAC to write into DDR and conversely the a CPU write to
    hit the MAC... but it underscores how at this level the problem is all hardware/topology,
    as hw latency is an order of magnitude larger than sw... no fun... so time to
    get back my day job.
  prefs: []
  type: TYPE_NORMAL
